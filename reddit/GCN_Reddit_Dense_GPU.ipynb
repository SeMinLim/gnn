{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "093d3101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Import torch & Check CUDA availability\n",
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ad20eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.cuda.device object at 0x7f5f0523a650>\n",
      "NVIDIA A30\n"
     ]
    }
   ],
   "source": [
    "# Get CUDA device name\n",
    "print(torch.cuda.device(0))\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b35c752c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: Reddit()\n",
      "-------------------\n",
      "Number of graphs: 1\n",
      "Number of nodes: 232965\n",
      "Number of features: 602\n",
      "Number of classes: 41\n",
      "\n",
      "Graph:\n",
      "------\n",
      "Edges are directed: False\n",
      "Graph has isolated nodes: False\n",
      "Graph has loops: False\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import Reddit\n",
    "\n",
    "# Import dataset from PyTorch Geometric\n",
    "dataset = Reddit(root=\"/dfs6/pub/seminl1/Reddit\")\n",
    "data = dataset[0]\n",
    "\n",
    "# Store the dataset to GPU\n",
    "data = data.pin_memory()\n",
    "data = data.to('cuda:0', non_blocking=True)\n",
    "\n",
    "# Print information about the dataset\n",
    "print(f'Dataset: {dataset}')\n",
    "print('-------------------')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of nodes: {data.x.shape[0]}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')\n",
    "\n",
    "# Print information about the graph\n",
    "print(f'\\nGraph:')\n",
    "print('------')\n",
    "print(f'Edges are directed: {data.is_directed()}')\n",
    "print(f'Graph has isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Graph has loops: {data.has_self_loops()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "134d5de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph is stored on the GPU: True\n"
     ]
    }
   ],
   "source": [
    "# Check whether the dataset is stored on the GPU or not\n",
    "print(f'Graph is stored on the GPU: {data.is_cuda}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46f36e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph: Data(x=[232965, 602], edge_index=[2, 114615892], y=[232965], train_mask=[232965], val_mask=[232965], test_mask=[232965])\n"
     ]
    }
   ],
   "source": [
    "# Print first element\n",
    "print(f'Graph: {data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1af89298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = torch.Size([232965, 602])\n",
      "torch.float32\n",
      "tensor([[ 1.2334,  9.0430, -0.9233,  ..., -0.2579,  0.3112, -0.3772],\n",
      "        [-0.1386, -0.2022,  0.1277,  ...,  0.1563,  0.1048, -0.6534],\n",
      "        [-0.1330, -0.1962, -0.0296,  ...,  0.0358,  0.2864,  0.2744],\n",
      "        ...,\n",
      "        [-0.0614, -0.2022,  0.9698,  ...,  1.1064, -1.4323, -0.2398],\n",
      "        [-0.1606, -0.2022, -0.0892,  ...,  0.7440, -0.5046, -2.2288],\n",
      "        [ 0.0929,  0.2822,  0.1768,  ...,  0.2196,  0.5967,  0.5588]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Node feature matrix information\n",
    "print(f'x = {data.x.shape}')\n",
    "print(data.x.dtype)\n",
    "print(data.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e041a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edge_index = torch.Size([2, 114615892])\n",
      "torch.int64\n",
      "tensor([[     0,      0,      0,  ..., 232964, 232964, 232964],\n",
      "        [   242,    249,    524,  ..., 231806, 232594, 232634]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Edge index information\n",
    "print(f'edge_index = {data.edge_index.shape}')\n",
    "print(data.edge_index.dtype)\n",
    "print(data.edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58d5aa52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0, device='cuda:0')\n",
      "tensor(1000, device='cuda:0')\n",
      "tensor(2000, device='cuda:0')\n",
      "tensor(3000, device='cuda:0')\n",
      "tensor(4000, device='cuda:0')\n",
      "tensor(5000, device='cuda:0')\n",
      "tensor(6000, device='cuda:0')\n",
      "tensor(7000, device='cuda:0')\n",
      "tensor(8000, device='cuda:0')\n",
      "tensor(9000, device='cuda:0')\n",
      "tensor(10000, device='cuda:0')\n",
      "tensor(11000, device='cuda:0')\n",
      "tensor(12000, device='cuda:0')\n",
      "tensor(13000, device='cuda:0')\n",
      "tensor(14000, device='cuda:0')\n",
      "tensor(15000, device='cuda:0')\n",
      "tensor(16000, device='cuda:0')\n",
      "tensor(17000, device='cuda:0')\n",
      "tensor(18000, device='cuda:0')\n",
      "tensor(19000, device='cuda:0')\n",
      "tensor(20000, device='cuda:0')\n",
      "tensor(21000, device='cuda:0')\n",
      "tensor(22000, device='cuda:0')\n",
      "tensor(23000, device='cuda:0')\n",
      "tensor(24000, device='cuda:0')\n",
      "tensor(25000, device='cuda:0')\n",
      "tensor(26000, device='cuda:0')\n",
      "tensor(27000, device='cuda:0')\n",
      "tensor(28000, device='cuda:0')\n",
      "tensor(29000, device='cuda:0')\n",
      "tensor(30000, device='cuda:0')\n",
      "tensor(31000, device='cuda:0')\n",
      "tensor(32000, device='cuda:0')\n",
      "tensor(33000, device='cuda:0')\n",
      "tensor(34000, device='cuda:0')\n",
      "tensor(35000, device='cuda:0')\n",
      "tensor(36000, device='cuda:0')\n",
      "tensor(37000, device='cuda:0')\n",
      "tensor(38000, device='cuda:0')\n",
      "tensor(39000, device='cuda:0')\n",
      "tensor(40000, device='cuda:0')\n",
      "tensor(41000, device='cuda:0')\n",
      "tensor(42000, device='cuda:0')\n",
      "tensor(43000, device='cuda:0')\n",
      "tensor(44000, device='cuda:0')\n",
      "tensor(45000, device='cuda:0')\n",
      "tensor(46000, device='cuda:0')\n",
      "tensor(47000, device='cuda:0')\n",
      "tensor(48000, device='cuda:0')\n",
      "tensor(49000, device='cuda:0')\n",
      "tensor(50000, device='cuda:0')\n",
      "tensor(51000, device='cuda:0')\n",
      "tensor(52000, device='cuda:0')\n",
      "tensor(53000, device='cuda:0')\n",
      "tensor(54000, device='cuda:0')\n",
      "tensor(55000, device='cuda:0')\n",
      "tensor(56000, device='cuda:0')\n",
      "tensor(57000, device='cuda:0')\n",
      "tensor(58000, device='cuda:0')\n",
      "tensor(59000, device='cuda:0')\n",
      "tensor(60000, device='cuda:0')\n",
      "tensor(61000, device='cuda:0')\n",
      "tensor(62000, device='cuda:0')\n",
      "tensor(63000, device='cuda:0')\n",
      "tensor(64000, device='cuda:0')\n",
      "tensor(65000, device='cuda:0')\n",
      "tensor(66000, device='cuda:0')\n",
      "tensor(67000, device='cuda:0')\n",
      "tensor(68000, device='cuda:0')\n",
      "tensor(69000, device='cuda:0')\n",
      "tensor(70000, device='cuda:0')\n",
      "tensor(71000, device='cuda:0')\n",
      "tensor(72000, device='cuda:0')\n",
      "tensor(73000, device='cuda:0')\n",
      "tensor(74000, device='cuda:0')\n",
      "tensor(75000, device='cuda:0')\n",
      "tensor(76000, device='cuda:0')\n",
      "tensor(77000, device='cuda:0')\n",
      "tensor(78000, device='cuda:0')\n",
      "tensor(79000, device='cuda:0')\n",
      "tensor(80000, device='cuda:0')\n",
      "tensor(81000, device='cuda:0')\n",
      "tensor(82000, device='cuda:0')\n",
      "tensor(83000, device='cuda:0')\n",
      "tensor(84000, device='cuda:0')\n",
      "tensor(85000, device='cuda:0')\n",
      "tensor(86000, device='cuda:0')\n",
      "tensor(87000, device='cuda:0')\n",
      "tensor(88000, device='cuda:0')\n",
      "tensor(89000, device='cuda:0')\n",
      "tensor(90000, device='cuda:0')\n",
      "tensor(91000, device='cuda:0')\n",
      "tensor(92000, device='cuda:0')\n",
      "tensor(93000, device='cuda:0')\n",
      "tensor(94000, device='cuda:0')\n",
      "tensor(95000, device='cuda:0')\n",
      "tensor(96000, device='cuda:0')\n",
      "tensor(97000, device='cuda:0')\n",
      "tensor(98000, device='cuda:0')\n",
      "tensor(99000, device='cuda:0')\n",
      "tensor(100000, device='cuda:0')\n",
      "tensor(101000, device='cuda:0')\n",
      "tensor(102000, device='cuda:0')\n",
      "tensor(103000, device='cuda:0')\n",
      "tensor(104000, device='cuda:0')\n",
      "tensor(105000, device='cuda:0')\n",
      "tensor(106000, device='cuda:0')\n",
      "tensor(107000, device='cuda:0')\n",
      "tensor(108000, device='cuda:0')\n",
      "tensor(109000, device='cuda:0')\n",
      "tensor(110000, device='cuda:0')\n",
      "tensor(111000, device='cuda:0')\n",
      "tensor(112000, device='cuda:0')\n",
      "tensor(113000, device='cuda:0')\n",
      "tensor(114000, device='cuda:0')\n",
      "tensor(115000, device='cuda:0')\n",
      "tensor(116000, device='cuda:0')\n",
      "tensor(117000, device='cuda:0')\n",
      "tensor(118000, device='cuda:0')\n",
      "tensor(119000, device='cuda:0')\n",
      "tensor(120000, device='cuda:0')\n",
      "tensor(121000, device='cuda:0')\n",
      "tensor(122000, device='cuda:0')\n",
      "tensor(123000, device='cuda:0')\n",
      "tensor(124000, device='cuda:0')\n",
      "tensor(125000, device='cuda:0')\n",
      "tensor(126000, device='cuda:0')\n",
      "tensor(127000, device='cuda:0')\n",
      "tensor(128000, device='cuda:0')\n",
      "tensor(129000, device='cuda:0')\n",
      "tensor(130000, device='cuda:0')\n",
      "tensor(131000, device='cuda:0')\n",
      "tensor(132000, device='cuda:0')\n",
      "tensor(133000, device='cuda:0')\n",
      "tensor(134000, device='cuda:0')\n",
      "tensor(135000, device='cuda:0')\n",
      "tensor(136000, device='cuda:0')\n",
      "tensor(137000, device='cuda:0')\n",
      "tensor(138000, device='cuda:0')\n",
      "tensor(139000, device='cuda:0')\n",
      "tensor(140000, device='cuda:0')\n",
      "tensor(141000, device='cuda:0')\n",
      "tensor(142000, device='cuda:0')\n",
      "tensor(143000, device='cuda:0')\n",
      "tensor(144000, device='cuda:0')\n",
      "tensor(145000, device='cuda:0')\n",
      "tensor(146000, device='cuda:0')\n",
      "tensor(147000, device='cuda:0')\n",
      "tensor(148000, device='cuda:0')\n",
      "tensor(149000, device='cuda:0')\n",
      "tensor(150000, device='cuda:0')\n",
      "tensor(151000, device='cuda:0')\n",
      "tensor(152000, device='cuda:0')\n",
      "tensor(153000, device='cuda:0')\n",
      "tensor(154000, device='cuda:0')\n",
      "tensor(155000, device='cuda:0')\n",
      "tensor(156000, device='cuda:0')\n",
      "tensor(157000, device='cuda:0')\n",
      "tensor(158000, device='cuda:0')\n",
      "tensor(159000, device='cuda:0')\n",
      "tensor(160000, device='cuda:0')\n",
      "tensor(161000, device='cuda:0')\n",
      "tensor(162000, device='cuda:0')\n",
      "tensor(163000, device='cuda:0')\n",
      "tensor(164000, device='cuda:0')\n",
      "tensor(165000, device='cuda:0')\n",
      "tensor(166000, device='cuda:0')\n",
      "tensor(167000, device='cuda:0')\n",
      "tensor(168000, device='cuda:0')\n",
      "tensor(169000, device='cuda:0')\n",
      "tensor(170000, device='cuda:0')\n",
      "tensor(171000, device='cuda:0')\n",
      "tensor(172000, device='cuda:0')\n",
      "tensor(173000, device='cuda:0')\n",
      "tensor(174000, device='cuda:0')\n",
      "tensor(175000, device='cuda:0')\n",
      "tensor(176000, device='cuda:0')\n",
      "tensor(177000, device='cuda:0')\n",
      "tensor(178000, device='cuda:0')\n",
      "tensor(179000, device='cuda:0')\n",
      "tensor(180000, device='cuda:0')\n",
      "tensor(181000, device='cuda:0')\n",
      "tensor(182000, device='cuda:0')\n",
      "tensor(183000, device='cuda:0')\n",
      "tensor(184000, device='cuda:0')\n",
      "tensor(185000, device='cuda:0')\n",
      "tensor(186000, device='cuda:0')\n",
      "tensor(187000, device='cuda:0')\n",
      "tensor(188000, device='cuda:0')\n",
      "tensor(189000, device='cuda:0')\n",
      "tensor(190000, device='cuda:0')\n",
      "tensor(191000, device='cuda:0')\n",
      "tensor(192000, device='cuda:0')\n",
      "tensor(193000, device='cuda:0')\n",
      "tensor(194000, device='cuda:0')\n",
      "tensor(195000, device='cuda:0')\n",
      "tensor(196000, device='cuda:0')\n",
      "tensor(197000, device='cuda:0')\n",
      "tensor(198000, device='cuda:0')\n",
      "tensor(199000, device='cuda:0')\n",
      "tensor(200000, device='cuda:0')\n",
      "tensor(201000, device='cuda:0')\n",
      "tensor(202000, device='cuda:0')\n",
      "tensor(203000, device='cuda:0')\n",
      "tensor(204000, device='cuda:0')\n",
      "tensor(205000, device='cuda:0')\n",
      "tensor(206000, device='cuda:0')\n",
      "tensor(207000, device='cuda:0')\n",
      "tensor(208000, device='cuda:0')\n",
      "tensor(209000, device='cuda:0')\n",
      "tensor(210000, device='cuda:0')\n",
      "tensor(211000, device='cuda:0')\n",
      "tensor(212000, device='cuda:0')\n",
      "tensor(213000, device='cuda:0')\n",
      "tensor(214000, device='cuda:0')\n",
      "tensor(215000, device='cuda:0')\n",
      "tensor(216000, device='cuda:0')\n",
      "tensor(217000, device='cuda:0')\n",
      "tensor(218000, device='cuda:0')\n",
      "tensor(219000, device='cuda:0')\n",
      "tensor(220000, device='cuda:0')\n",
      "tensor(221000, device='cuda:0')\n",
      "tensor(222000, device='cuda:0')\n",
      "tensor(223000, device='cuda:0')\n",
      "tensor(224000, device='cuda:0')\n",
      "tensor(225000, device='cuda:0')\n",
      "tensor(226000, device='cuda:0')\n",
      "tensor(227000, device='cuda:0')\n",
      "tensor(228000, device='cuda:0')\n",
      "tensor(229000, device='cuda:0')\n",
      "tensor(230000, device='cuda:0')\n",
      "tensor(231000, device='cuda:0')\n",
      "tensor(232000, device='cuda:0')\n",
      "114382927\n"
     ]
    }
   ],
   "source": [
    "# Try to get the number of addition operation\n",
    "start_point = 0\n",
    "numAddition = 0\n",
    "numEdges = 114615892\n",
    "for i in range(numEdges-1):\n",
    "    if data.edge_index[0][i+1] == start_point:\n",
    "        numAddition += 1\n",
    "    else:\n",
    "        start_point = data.edge_index[0][i+1]\n",
    "        if data.edge_index[0][i] % 1000 == 0:\n",
    "            print(data.edge_index[0][i])\n",
    "print(numAddition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a8a9119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = torch.Size([232965])\n",
      "torch.int64\n",
      "tensor([30, 17, 18,  ...,  3, 13, 13], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Ground-truth labels\n",
    "print(f'y = {data.y.shape}')\n",
    "print(data.y.dtype)\n",
    "print(data.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01511594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_mask = torch.Size([232965])\n",
      "torch.bool\n",
      "tensor([False,  True, False,  ...,  True,  True, False], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Train mask\n",
    "print(f'train_mask = {data.train_mask.shape}')\n",
    "print(data.train_mask.dtype)\n",
    "print(data.train_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e574a813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple GCN with only one GCN layer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.gcn1 = GCNConv(dataset.num_features, dataset.num_classes)\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(),\n",
    "                                          lr=0.01,\n",
    "                                          weight_decay=5e-4)\n",
    "\n",
    "    def forward(self, x, adj_t):\n",
    "        x = self.gcn1(x, adj_t)\n",
    "        z = F.log_softmax(x, dim=1)\n",
    "        return x, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40b257a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred_y, y):\n",
    "    \"\"\"Calculate accuracy.\"\"\"\n",
    "    return ((pred_y == y).sum() / len(y)).item()\n",
    "\n",
    "def train(model, data):\n",
    "    \"\"\"Train a GNN model and return the trained model.\"\"\"\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = model.optimizer\n",
    "    epochs = 200\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs+1):\n",
    "        # Training\n",
    "        optimizer.zero_grad()\n",
    "        h, out = model(data.x, data.edge_index)\n",
    "        loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "        acc = accuracy(out[data.train_mask].argmax(dim=1), data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print metrics every 10 epochs\n",
    "        if(epoch % 10 == 0):\n",
    "            print(f'Epoch {epoch:>3} | Train Loss: {loss:.3f} | Train Acc: {acc*100:>6.2f}%')\n",
    "          \n",
    "    return model, h, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ac12626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (gcn1): GCNConv(602, 41)\n",
      ")\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 17.54 GiB. GPU 0 has a total capacty of 23.50 GiB of which 17.43 GiB is free. Including non-PyTorch memory, this process has 6.05 GiB memory in use. Of the allocated memory 4.42 GiB is allocated by PyTorch, and 1.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:8\u001b[0m\n",
      "Cell \u001b[0;32mIn[11], line 15\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, data)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 15\u001b[0m     h, out \u001b[38;5;241m=\u001b[39m model(data\u001b[38;5;241m.\u001b[39mx, data\u001b[38;5;241m.\u001b[39medge_index)\n\u001b[1;32m     16\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(out[data\u001b[38;5;241m.\u001b[39mtrain_mask], data\u001b[38;5;241m.\u001b[39my[data\u001b[38;5;241m.\u001b[39mtrain_mask])\n\u001b[1;32m     17\u001b[0m     acc \u001b[38;5;241m=\u001b[39m accuracy(out[data\u001b[38;5;241m.\u001b[39mtrain_mask]\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), data\u001b[38;5;241m.\u001b[39my[data\u001b[38;5;241m.\u001b[39mtrain_mask])\n",
      "File \u001b[0;32m~/.conda/envs/A30/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/A30/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 16\u001b[0m, in \u001b[0;36mGCN.forward\u001b[0;34m(self, x, adj_t)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, adj_t):\n\u001b[0;32m---> 16\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgcn1(x, adj_t)\n\u001b[1;32m     17\u001b[0m     z \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mlog_softmax(x, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x, z\n",
      "File \u001b[0;32m~/.conda/envs/A30/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/A30/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/A30/lib/python3.11/site-packages/torch_geometric/nn/conv/gcn_conv.py:244\u001b[0m, in \u001b[0;36mGCNConv.forward\u001b[0;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[1;32m    241\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin(x)\n\u001b[1;32m    243\u001b[0m \u001b[38;5;66;03m# propagate_type: (x: Tensor, edge_weight: OptTensor)\u001b[39;00m\n\u001b[0;32m--> 244\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpropagate(edge_index, x\u001b[38;5;241m=\u001b[39mx, edge_weight\u001b[38;5;241m=\u001b[39medge_weight,\n\u001b[1;32m    245\u001b[0m                      size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    248\u001b[0m     out \u001b[38;5;241m=\u001b[39m out \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\n",
      "File \u001b[0;32m~/.conda/envs/A30/lib/python3.11/site-packages/torch_geometric/nn/conv/message_passing.py:455\u001b[0m, in \u001b[0;36mMessagePassing.propagate\u001b[0;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[1;32m    452\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m decomp_args:\n\u001b[1;32m    453\u001b[0m         kwargs[arg] \u001b[38;5;241m=\u001b[39m decomp_kwargs[arg][i]\n\u001b[0;32m--> 455\u001b[0m coll_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_collect(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_user_args, edge_index, size,\n\u001b[1;32m    456\u001b[0m                           kwargs)\n\u001b[1;32m    458\u001b[0m msg_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minspector\u001b[38;5;241m.\u001b[39mdistribute(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m, coll_dict)\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_message_forward_pre_hooks\u001b[38;5;241m.\u001b[39mvalues():\n",
      "File \u001b[0;32m~/.conda/envs/A30/lib/python3.11/site-packages/torch_geometric/nn/conv/message_passing.py:329\u001b[0m, in \u001b[0;36mMessagePassing._collect\u001b[0;34m(self, args, edge_index, size, kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, Tensor):\n\u001b[1;32m    328\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_size(size, dim, data)\n\u001b[0;32m--> 329\u001b[0m             data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lift(data, edge_index, dim)\n\u001b[1;32m    331\u001b[0m         out[arg] \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_sparse_tensor(edge_index):\n",
      "File \u001b[0;32m~/.conda/envs/A30/lib/python3.11/site-packages/torch_geometric/nn/conv/message_passing.py:276\u001b[0m, in \u001b[0;36mMessagePassing._lift\u001b[0;34m(self, src, edge_index, dim)\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\n\u001b[1;32m    270\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered an index error. Please ensure that all \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    271\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindices in \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_index\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m point to valid indices in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    272\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe interval [0, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msrc\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_dim)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    273\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(got interval \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    274\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(index\u001b[38;5;241m.\u001b[39mmin())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(index\u001b[38;5;241m.\u001b[39mmax())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m])\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m index\u001b[38;5;241m.\u001b[39mmin() \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound negative indices in \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_index\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex\u001b[38;5;241m.\u001b[39mmin()\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m). Please ensure that all \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindices in \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_index\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m point to valid indices \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min the interval [0, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msrc\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_dim)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour node feature matrix and try again.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/A30/lib/python3.11/site-packages/torch_geometric/nn/conv/message_passing.py:266\u001b[0m, in \u001b[0;36mMessagePassing._lift\u001b[0;34m(self, src, edge_index, dim)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    265\u001b[0m     index \u001b[38;5;241m=\u001b[39m edge_index[dim]\n\u001b[0;32m--> 266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m src\u001b[38;5;241m.\u001b[39mindex_select(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_dim, index)\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mIndexError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index\u001b[38;5;241m.\u001b[39mmin() \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m index\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_dim):\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 17.54 GiB. GPU 0 has a total capacty of 23.50 GiB of which 17.43 GiB is free. Including non-PyTorch memory, this process has 6.05 GiB memory in use. Of the allocated memory 4.42 GiB is allocated by PyTorch, and 1.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create GCN model\n",
    "gcn = GCN()\n",
    "print(gcn)\n",
    "\n",
    "# Train and test\n",
    "# Train\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "gcn_model, gcn_output, final_output = train(gcn.to(device), data.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41f9aa9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
